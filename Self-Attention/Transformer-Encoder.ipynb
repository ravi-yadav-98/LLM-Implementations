{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6d3daa-54f2-4d21-98e8-1e073c743a4d",
   "metadata": {},
   "source": [
    "## Encoder in Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c0bdba-50aa-4351-bd99-1eddd58bb0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div id=\"mKnpmc\"></div>\n",
       "            <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "                if(!window.letsPlotCallQueue) {\n",
       "                    window.letsPlotCallQueue = [];\n",
       "                }; \n",
       "                window.letsPlotCall = function(f) {\n",
       "                    window.letsPlotCallQueue.push(f);\n",
       "                };\n",
       "                (function() {\n",
       "                    var script = document.createElement(\"script\");\n",
       "                    script.type = \"text/javascript\";\n",
       "                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v4.5.1/js-package/distr/lets-plot.min.js\";\n",
       "                    script.onload = function() {\n",
       "                        window.letsPlotCall = function(f) {f();};\n",
       "                        window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        \n",
       "                    };\n",
       "                    script.onerror = function(event) {\n",
       "                        window.letsPlotCall = function(f) {};    // noop\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        var div = document.createElement(\"div\");\n",
       "                        div.style.color = 'darkred';\n",
       "                        div.textContent = 'Error loading Lets-Plot JS';\n",
       "                        document.getElementById(\"mKnpmc\").appendChild(div);\n",
       "                    };\n",
       "                    var e = document.getElementById(\"mKnpmc\");\n",
       "                    e.appendChild(script);\n",
       "                })()\n",
       "            </script>\n",
       "            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import some libraries we'll probably use\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# just used for plotting\n",
    "from lets_plot import *\n",
    "LetsPlot.setup_html()\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, embed_dim: int, n_heads: int, dim_feedforward: int = 128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.mha = torch.nn.MultiheadAttention(embed_dim=embed_dim, num_heads=n_heads, batch_first=True, bias=True)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(normalized_shape=embed_dim)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(normalized_shape=embed_dim)\n",
    "\n",
    "        # section 5.4\n",
    "        # apply dropout to output of each sublayer before it is added to sublayer's input\n",
    "        self.dropout1 = torch.nn.Dropout(p=dropout)\n",
    "        self.dropout2 = torch.nn.Dropout(p=dropout)\n",
    "        \n",
    "        # section 3.3 in paper\n",
    "        self.position_wise_ff = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=embed_dim, out_features=dim_feedforward, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=dim_feedforward, out_features=embed_dim, bias=True)\n",
    "        )\n",
    "    def forward(self, x, src_key_padding_mask=None, src_mask=None):\n",
    "        # x.shape = (batch_size, seq_len, embed_dim)\n",
    "        # src_key_padding_mask = (bs, seq_len), True value indicates it should not attend\n",
    "        # src_mask.shape = (bs, seq_len, seq_len) of dtype torch.bool, True value indicates it shouldn't attend\n",
    "        attn_output, attn_weights = self.mha(x, x, x, key_padding_mask=src_key_padding_mask, attn_mask=src_mask)\n",
    "        # dropout and residual connection\n",
    "        x  = x + self.dropout1(attn_output)\n",
    "        x = self.layer_norm1(x)\n",
    "        \n",
    "        projection = self.position_wise_ff(x)\n",
    "        # dropout and residual connection\n",
    "        x = x + self.dropout2(projection)\n",
    "        # layer norm\n",
    "        x = self.layer_norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3894ba-2b3f-4aac-82f4-1c0a08bdf001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
