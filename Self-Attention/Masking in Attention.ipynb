{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2c55b7-fff8-4489-b224-d1db21066d31",
   "metadata": {},
   "source": [
    "### Masking in Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7018d39-d66a-4948-926b-47c8be464ae9",
   "metadata": {},
   "source": [
    "#### 1. Casual Attention Mask : Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4548ce59-868a-4679-936b-0a23c1039fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3844, 0.3415, 0.2741],\n",
      "         [0.2364, 0.2711, 0.4925],\n",
      "         [0.3552, 0.3550, 0.2898]],\n",
      "\n",
      "        [[0.4541, 0.5459, 0.0000],\n",
      "         [0.4614, 0.5386, 0.0000],\n",
      "         [0.4634, 0.5366, 0.0000]]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "embed_dim = 4\n",
    "mha = torch.nn.MultiheadAttention(embed_dim=embed_dim, num_heads=1, batch_first=True)\n",
    "\n",
    "# assume we have a batch of 2 sentences. 1st has 3 tokens and 2nd has 2 tokens\n",
    "embeddings = torch.normal(mean=0, std=1, size=(2, 3, embed_dim))\n",
    "# create a padding mask with all zeros so that every token is valid by default\n",
    "key_padding_mask = torch.zeros(size=(2, 3), dtype=torch.bool)\n",
    "# 3rd token of second sentence is a pad token\n",
    "key_padding_mask[1, 2] = 1\n",
    "\n",
    "_, torch_attn_mask = mha(embeddings, embeddings, embeddings, key_padding_mask=key_padding_mask)\n",
    "print(torch_attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8776eed-8644-416f-9ccd-1ab9715928b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8123e-01,  3.6679e-01, -1.3186e+00, -3.0594e-01],\n",
       "         [ 2.7683e-01, -6.5081e-01,  2.2066e-02, -1.2154e+00],\n",
       "         [-1.1054e+00,  8.5865e-01,  5.0024e-01,  5.5196e-01]],\n",
       "\n",
       "        [[-1.0171e+00, -2.0578e-01, -9.2888e-01, -1.8757e+00],\n",
       "         [-1.2949e+00, -8.5911e-01,  6.2310e-04, -1.0198e+00],\n",
       "         [-6.9919e-01, -3.5184e-01,  1.5795e+00,  7.6255e-01]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38865173-3b0c-4572-9cfb-40cb9ef1f4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [False, False,  True]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_padding_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b18bc13-7d0f-41ee-adae-9bd1dd22fa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False, False],\n",
      "         [False, False, False],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False, False,  True],\n",
      "         [False, False,  True],\n",
      "         [False, False,  True]]])\n"
     ]
    }
   ],
   "source": [
    "# reshape mask to proper shape\n",
    "key_padding_mask_expanded = key_padding_mask.unsqueeze(1) # (bs, 1, seq_len)\n",
    "# expand 3 times in the 2nd dimension since we have 3 tokens\n",
    "key_padding_mask_expanded = key_padding_mask_expanded.expand(-1, 3, -1)\n",
    "print(key_padding_mask_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e003aba-91f4-416f-945b-c07836effc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.9997,  0.1542, -0.7139],\n",
      "         [ 0.1542,  1.9779, -1.5246],\n",
      "         [-0.7139, -1.5246,  2.5140]],\n",
      "\n",
      "        [[ 5.4580,  3.4061, -2.1140],\n",
      "         [ 3.4061,  3.4548,  0.4310],\n",
      "         [-2.1140,  0.4310,  3.6889]]])\n",
      "tensor([[[ 1.9997,  0.1542, -0.7139],\n",
      "         [ 0.1542,  1.9779, -1.5246],\n",
      "         [-0.7139, -1.5246,  2.5140]],\n",
      "\n",
      "        [[ 5.4580,  3.4061,    -inf],\n",
      "         [ 3.4061,  3.4548,    -inf],\n",
      "         [-2.1140,  0.4310,    -inf]]])\n",
      "tensor([[[0.8200, 0.1300, 0.0500],\n",
      "         [0.1400, 0.8400, 0.0300],\n",
      "         [0.0400, 0.0200, 0.9500]],\n",
      "\n",
      "        [[0.8900, 0.1100, 0.0000],\n",
      "         [0.4900, 0.5100, 0.0000],\n",
      "         [0.0700, 0.9300, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# compute dot-product between Query and Key tokens\n",
    "scores = embeddings @ embeddings.transpose(1, 2)\n",
    "print(scores)\n",
    "# where ever the mask value is True, fill the corresponding entry in scores to -inf\n",
    "scores = scores.masked_fill(key_padding_mask_expanded, -torch.inf)\n",
    "print(scores)\n",
    "attn_weights = torch.softmax(scores, dim=-1)\n",
    "print(attn_weights.round(decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99bf2116-1088-46fa-9305-0c9842085dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8200, 0.1300, 0.0500],\n",
      "         [0.1400, 0.8400, 0.0300],\n",
      "         [0.0400, 0.0200, 0.9500]],\n",
      "\n",
      "        [[0.8900, 0.1100, 0.0000],\n",
      "         [0.4900, 0.5100, 0.0000],\n",
      "         [0.0700, 0.9300, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "scores = embeddings @ embeddings.transpose(1, 2)\n",
    "# create a float_mask as I describe previously\n",
    "float_mask = torch.zeros_like(key_padding_mask_expanded, dtype=torch.float32).masked_fill(key_padding_mask_expanded, -torch.inf)\n",
    "# add the float mask to the scores and apply softmax function\n",
    "print(torch.softmax(scores + float_mask, dim=-1).round(decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca5c9180-ca1a-4e32-907a-d8d223ce1321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False,  True,  True],\n",
      "         [False, False,  True],\n",
      "         [False, False, False]],\n",
      "\n",
      "        [[False,  True,  True],\n",
      "         [False, False,  True],\n",
      "         [False, False, False]]])\n",
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.4658, 0.5342, 0.0000],\n",
      "         [0.3552, 0.3550, 0.2898]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000],\n",
      "         [0.4614, 0.5386, 0.0000],\n",
      "         [0.3019, 0.3496, 0.3485]]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# we have 2 sentences and 3 tokens\n",
    "causal_mask = torch.ones((2, 3, 3), dtype=torch.bool)\n",
    "causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "print(causal_mask)\n",
    "print(mha(embeddings, embeddings, embeddings, attn_mask=causal_mask)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44e1863d-e1a5-4e29-adff-233f645ca155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.4658, 0.5342, 0.0000],\n",
      "         [0.3552, 0.3550, 0.2898]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000],\n",
      "         [0.4614, 0.5386, 0.0000],\n",
      "         [0.3019, 0.3496, 0.3485]]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "causal_mask = torch.nn.Transformer.generate_square_subsequent_mask(sz=3) # we have 3 tokens, so size=3\n",
    "print(mha(embeddings, embeddings, embeddings, attn_mask=causal_mask)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f9d0c2e-8cea-4a7b-9fc6-a2a62a95f1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf],\n",
       "        [0., 0., -inf],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b7b5f-e1b4-40be-91ea-7021b5e46d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
