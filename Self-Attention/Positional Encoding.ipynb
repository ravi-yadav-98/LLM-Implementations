{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d666e0f-73ce-4834-b183-03d613e6f55c",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "Positional encoding is a vector added to the input embeddings (word vectors) at each position. \n",
    "This vector is designed in such a way that it allows the model to easily figure out the relative and absolute positions of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254cc2a-94ed-4ed0-a60c-697dc8ef03ac",
   "metadata": {},
   "source": [
    "### Positional Encoding Formula\n",
    "\n",
    "For a token at position \\( \\text{pos} \\) and embedding dimension \\( i \\), the positional encoding is computed as:\n",
    "\n",
    "#### For even dimensions (\\( 2i \\)):\n",
    "$$\n",
    "PE_{\\text{(pos, 2i)}} = \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "#### For odd dimensions (\\( 2i+1 \\)):\n",
    "$$\n",
    "PE_{\\text{(pos, 2i+1)}} = \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Where:\n",
    "- \\( \\text{pos} \\): The position index of the token in the sequence.\n",
    "- \\( i \\): The embedding dimension index.\n",
    "- \\( d \\): The total embedding dimension size (e.g., 512).\n",
    "- \\( 10000 \\): A scaling factor to control the frequency of the sine and cosine functions.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points:\n",
    "1. Sine is applied to **even** dimensions, and cosine is applied to **odd** dimensions.\n",
    "2. The denominator \\( 10000^{\\frac{2i}{d}} \\) controls the frequency of the oscillations:\n",
    "   - **Low-frequency** components for smaller \\( i \\).\n",
    "   - **High-frequency** components for larger \\( i \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e92023-34a1-45b3-9610-bf598c25f076",
   "metadata": {},
   "source": [
    "### Simplest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97333b34-a9e3-4a78-8f84-23b824dfcea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding Matrix:\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9996e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01],\n",
      "        [-9.5892e-01,  2.8366e-01,  4.7943e-01,  8.7758e-01,  4.9979e-02,\n",
      "          9.9875e-01,  5.0000e-03,  9.9999e-01],\n",
      "        [-2.7942e-01,  9.6017e-01,  5.6464e-01,  8.2534e-01,  5.9964e-02,\n",
      "          9.9820e-01,  6.0000e-03,  9.9998e-01],\n",
      "        [ 6.5699e-01,  7.5390e-01,  6.4422e-01,  7.6484e-01,  6.9943e-02,\n",
      "          9.9755e-01,  6.9999e-03,  9.9998e-01],\n",
      "        [ 9.8936e-01, -1.4550e-01,  7.1736e-01,  6.9671e-01,  7.9915e-02,\n",
      "          9.9680e-01,  7.9999e-03,  9.9997e-01],\n",
      "        [ 4.1212e-01, -9.1113e-01,  7.8333e-01,  6.2161e-01,  8.9879e-02,\n",
      "          9.9595e-01,  8.9999e-03,  9.9996e-01]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Computes positional encodings using loops.\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): Length of the sequence (number of tokens).\n",
    "        d_model (int): Embedding dimension.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Positional encoding matrix of shape (seq_len, d_model).\n",
    "    \"\"\"\n",
    "    # Initialize positional encoding matrix\n",
    "    pos_enc = np.zeros((seq_len, d_model))\n",
    "    \n",
    "    # Loop over each position in the sequence\n",
    "    for pos in range(seq_len):\n",
    "        # Loop over each dimension in the embedding\n",
    "        for i in range(d_model):\n",
    "            # Compute the denominator term: 10000^(2*(i//2)/d_model)\n",
    "            denominator = 10000 ** ((2 * (i // 2)) / d_model)\n",
    "            \n",
    "            # Apply sine to even dimensions, cosine to odd dimensions\n",
    "            if i % 2 == 0:  # Even dimensions: 2i\n",
    "                pos_enc[pos, i] = np.sin(pos / denominator)\n",
    "            else:  # Odd dimensions: 2i+1\n",
    "                pos_enc[pos, i] = np.cos(pos / denominator)\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    return torch.tensor(pos_enc, dtype=torch.float32)\n",
    "\n",
    "# Example usage\n",
    "seq_len = 10  # Sequence length (number of tokens)\n",
    "d_model = 8   # Embedding dimension\n",
    "\n",
    "pos_encoding = positional_encoding(seq_len, d_model)\n",
    "print(\"Positional Encoding Matrix:\")\n",
    "print(pos_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3008322-d84c-46db-b53d-4de00dd18c57",
   "metadata": {},
   "source": [
    "### Efficient Implementation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50159a98-ac5c-4279-9120-23077de15992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding Matrix:\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9996e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01],\n",
      "        [-9.5892e-01,  2.8366e-01,  4.7943e-01,  8.7758e-01,  4.9979e-02,\n",
      "          9.9875e-01,  5.0000e-03,  9.9999e-01],\n",
      "        [-2.7942e-01,  9.6017e-01,  5.6464e-01,  8.2534e-01,  5.9964e-02,\n",
      "          9.9820e-01,  6.0000e-03,  9.9998e-01],\n",
      "        [ 6.5699e-01,  7.5390e-01,  6.4422e-01,  7.6484e-01,  6.9943e-02,\n",
      "          9.9755e-01,  6.9999e-03,  9.9998e-01],\n",
      "        [ 9.8936e-01, -1.4550e-01,  7.1736e-01,  6.9671e-01,  7.9915e-02,\n",
      "          9.9680e-01,  7.9999e-03,  9.9997e-01],\n",
      "        [ 4.1212e-01, -9.1113e-01,  7.8333e-01,  6.2161e-01,  8.9879e-02,\n",
      "          9.9595e-01,  8.9999e-03,  9.9996e-01]])\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Computes the positional encodings for a sequence of length seq_len and embedding dimension d_model.\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): Length of the input sequence.\n",
    "        d_model (int): Dimension of the embedding space.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Positional encoding matrix of shape (seq_len, d_model).\n",
    "    \"\"\"\n",
    "    # Initialize a positional encoding matrix (seq_len, d_model)\n",
    "    pos_enc = np.zeros((seq_len, d_model))\n",
    "    \n",
    "    # Get position indices (0 to seq_len-1)\n",
    "    positions = np.arange(0, seq_len)[:, np.newaxis]  # Shape: (seq_len, 1)\n",
    "    \n",
    "    # Get dimension indices (0 to d_model-1)\n",
    "    dimensions = np.arange(0, d_model)[np.newaxis, :]  # Shape: (1, d_model)\n",
    "    \n",
    "    # Compute the positional encoding formula\n",
    "    denominator = 10000 ** (2 * (dimensions // 2) / d_model)\n",
    "    pos_enc[:, 0::2] = np.sin(positions / denominator[:, 0::2])  # Apply sine to even indices\n",
    "    pos_enc[:, 1::2] = np.cos(positions / denominator[:, 1::2])  # Apply cosine to odd indices\n",
    "    \n",
    "    # Convert to PyTorch tensor for compatibility with deep learning frameworks\n",
    "    return torch.tensor(pos_enc, dtype=torch.float32)\n",
    "\n",
    "# Example usage\n",
    "seq_len = 10  # Sequence length (number of tokens)\n",
    "d_model = 8   # Embedding dimension\n",
    "\n",
    "pos_encoding = positional_encoding(seq_len, d_model)\n",
    "print(\"Positional Encoding Matrix:\")\n",
    "print(pos_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f354f-10a2-41bd-9067-f102b80c46b1",
   "metadata": {},
   "source": [
    "### PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e633a1-50ba-4a39-9af6-2e1a4265fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=256):\n",
    "        super().__init__()\n",
    "        # create a matrix of [seq_len, hidden_dim] representing positional encoding for each token in sequence\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6963ada5-bdff-4590-85a2-4a7f561457e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, 9, dtype=torch.float)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bec3ac-5c89-4ed8-b524-057f4007d054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
