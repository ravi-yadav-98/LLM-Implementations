{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5884810c-edcf-4ad3-b336-0178947ad269",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7330c3-f4eb-4d96-b6ba-f8ee25b25018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bffe578-74b6-4bde-a5ad-c79dad621e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_scaled_dot_product_attention(query, key=None, value=None):\n",
    "    key = key if key is not None else query\n",
    "    value = value if value is not None else query\n",
    "    # query and key must have same embedding dimension\n",
    "    assert query.size(-1) == key.size(-1)\n",
    "\n",
    "    dk = key.size(-1) # embed dimension of key\n",
    "    # query, key, value = (bs, seq_len, embed_dim)\n",
    "    \n",
    "    # compute dot-product to obtain pairwise \"similarity\" and scale it\n",
    "    qk = query @ key.transpose(-1, -2) / dk**0.5\n",
    "    \n",
    "    # apply softmax\n",
    "    # attn_weights = (bs, seq_len, seq_len)\n",
    "    attn_weights = torch.softmax(qk, dim=-1)\n",
    "\n",
    "    # compute weighted sum of value vectors\n",
    "    # attn = (bs, seq_len, embed_dim)\n",
    "    attn = attn_weights @ value\n",
    "    return attn, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022eb2cc-3cfe-45f4-84f3-7ceff7dc1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(mean=0, std=1, size=(2, 3, 6))\n",
    "torch_attended = torch.nn.functional.scaled_dot_product_attention(X, X, X)\n",
    "attended, attn_weights = my_scaled_dot_product_attention(X, X, X)\n",
    "assert torch.allclose(torch_attended, attended) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513dfd7a-7422-4812-8c2f-f96e6f65e122",
   "metadata": {},
   "source": [
    "#### Batch Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbc7197a-f15a-4484-ba8b-b30a2e3e52b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "A = torch.randn(batch_size, 10, 256)\n",
    "\n",
    "output = []\n",
    "for batch_idx in range(batch_size):\n",
    "    pairwise_dot_product = A[batch_idx] @ A[batch_idx].transpose(-1, -2)\n",
    "    output.append(pairwise_dot_product)\n",
    "\n",
    "# Output has shape (batch_size, 10, 10)\n",
    "output[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac2219-c6af-45a5-9809-96160d1c61de",
   "metadata": {},
   "source": [
    "### Naive Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a828e8e1-5ddb-4b30-b5de-9624db338459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, bias=False):\n",
    "        super().__init__()\n",
    "        # Linear layers to project Query, Key and Value \n",
    "        self.W_q = torch.nn.Linear(input_dim, output_dim, bias=bias)\n",
    "        self.W_k = torch.nn.Linear(input_dim, output_dim, bias=bias)\n",
    "        self.W_v = torch.nn.Linear(input_dim, output_dim, bias=bias)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # project Q, K, V\n",
    "        q_logits = self.W_q(query)\n",
    "        k_logits = self.W_k(key)\n",
    "        v_logits = self.W_v(value)\n",
    "\n",
    "        # apply scaled dot product attention on projected values\n",
    "        attn, weights = my_scaled_dot_product_attention(q_logits, k_logits, v_logits)\n",
    "        return attn, weights\n",
    "\n",
    "class MyMultiheadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim: int, n_heads: int, projection_bias=False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % n_heads == 0, \"embed_dim must be divisible by n_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        head_embed_dim = self.embed_dim // n_heads\n",
    "        # for each head, create an attention block\n",
    "        self.head_blocks = torch.nn.ModuleList([AttentionBlock(input_dim=embed_dim, output_dim=head_embed_dim, bias=projection_bias) for i in range(self.n_heads)])\n",
    "        # final projection of MHA\n",
    "        self.projection = torch.nn.Linear(embed_dim, embed_dim, bias=projection_bias)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # these lists are to store output of each head\n",
    "        attns_list = []\n",
    "        attn_weights_list = []\n",
    "\n",
    "        # for every head pass the original query, key, value\n",
    "        for head in self.head_blocks:\n",
    "            attn, attn_weights = head(query, key, value)\n",
    "            attns_list.append(attn)\n",
    "            attn_weights_list.append(attn_weights)\n",
    "\n",
    "        # concatenate attention outputs and take average of attention weights\n",
    "        attns, attn_weights = torch.cat(attns_list, dim=2), torch.stack(attn_weights_list).mean(dim=0)\n",
    "        # shape: (bs, seq_len, embed_dim), attn_weights: (bs, seq_len, seq_len)\n",
    "        return self.projection(attns), attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d786e-26ae-442a-942e-a04f0b10ea4d",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "056adc2b-7cbe-48bc-9356-edde08da2c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d20302ed9d485fbc7cfbb50c899e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/880 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\LLM-Implementations\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ryadav2\\.cache\\huggingface\\hub\\datasets--SetFit--bbc-news. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b7441fe5634424889cc9584ab7ff7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/2.87M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3c583cff3a4aefa57faaf9af107305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl:   0%|          | 0.00/2.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13937f28341742eda49b42d772695125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f18f561f4cf43ed8dbf5c139ebab685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "news_ds = datasets.load_dataset(\"SetFit/bbc-news\", split=\"train\")\n",
    "# train a new tokenizer with limited vocab size for demo\n",
    "tokenizer = original_tokenizer.train_new_from_iterator(news_ds['text'], vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c29d57f-a237-4dd5-93c9-e282b8d253d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c733a8e53c40d28b471cdaf89ef3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], truncation=True)\n",
    "\n",
    "ds = news_ds.map(tokenize, batched=True).select_columns(['label', 'input_ids', 'text']).train_test_split()\n",
    "\n",
    "class_id_to_class = {\n",
    "    0: \"tech\",\n",
    "    1: \"business\",\n",
    "    2: \"sports\",\n",
    "    3: \"entertainment\",\n",
    "    4: \"politics\",\n",
    "}\n",
    "num_classes = len(class_id_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "649e3c2b-8139-4646-9f5c-17619cef0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int, mha: torch.nn.Module):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)\n",
    "        self.mha = mha\n",
    "        self.fc1 = torch.nn.Linear(in_features=embed_dim, out_features=128)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.final = torch.nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, **kwargs):\n",
    "        # inputs: (bs, seq_len)\n",
    "        # embeddings: (bs, seq_len, embed_dim)\n",
    "        embeddings = self.get_embeddings(input_ids)\n",
    "        attn, attn_weights = self.get_attention(embeddings, embeddings, embeddings)\n",
    "        \n",
    "        # take the first token's embeddings i.e. embeddings of CLS token\n",
    "        # cls_token_embeddings: (bs, embed_dim)\n",
    "        cls_token_embeddings = attn[:, 0, :] \n",
    "        return self.final(self.relu(self.fc1(cls_token_embeddings)))\n",
    "    \n",
    "    def get_embeddings(self, input_ids):\n",
    "        return self.embedding(input_ids)\n",
    "    \n",
    "    def get_attention(self, query, key, value):\n",
    "        attn, attn_weights = self.mha(query, key, value)\n",
    "        return attn, attn_weights\n",
    "\n",
    "n_heads = 8\n",
    "embed_dim = 64\n",
    "vocab_size = tokenizer.vocab_size\n",
    "torch_mha = torch.nn.MultiheadAttention(embed_dim=embed_dim, num_heads=n_heads, batch_first=True)\n",
    "my_mha = MyMultiheadAttention(embed_dim=embed_dim, n_heads=n_heads, projection_bias=True)\n",
    "torch_classifier = TextClassifier(vocab_size=tokenizer.vocab_size, embed_dim=embed_dim, num_classes=num_classes, mha=torch_mha)\n",
    "my_classifier = TextClassifier(vocab_size=tokenizer.vocab_size, embed_dim=embed_dim, num_classes=num_classes, mha=my_mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "435fe3b6-bfb9-4595-b6d4-91840fc493a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "def collate_fn(batch):\n",
    "    labels = []\n",
    "    input_ids = []\n",
    "    for row in batch:\n",
    "        labels.append(row['label'])\n",
    "        input_ids.append(torch.LongTensor(row['input_ids']))\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    input_ids = torch.Tensor(input_ids)\n",
    "    return {\"labels\": labels, \"input_ids\": input_ids}\n",
    "\n",
    "train_dl = test_dl = DataLoader(ds['train'], shuffle=True, batch_size=32, collate_fn=collate_fn)\n",
    "test_dl = DataLoader(ds['test'], shuffle=False, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "def train(model: torch.nn.Module, train_dl, val_dl, epochs=10) -> list[tuple[float, float]]:\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    train_start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_dl:\n",
    "            optim.zero_grad()\n",
    "            logits = model(**batch)\n",
    "            loss = loss_fn(logits, batch['labels'])\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss += loss.item() * batch['labels'].size(0)\n",
    "\n",
    "        train_loss /= len(train_dl.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                logits = model(**batch)\n",
    "                loss = loss_fn(logits, batch['labels'])\n",
    "                val_loss += loss.item() * batch['labels'].size(0)\n",
    "                val_accuracy += (logits.argmax(dim=1) == batch['labels']).sum()\n",
    "\n",
    "        val_loss /= len(val_dl.dataset)\n",
    "        val_accuracy /= len(val_dl.dataset)\n",
    "        log_steps = max(1, int(0.2 * epochs))\n",
    "\n",
    "        losses.append((train_loss, val_loss))\n",
    "        if epoch % log_steps == 0 or epoch == epochs - 1:\n",
    "            epoch_duartion = time.time() - epoch_start\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}. Epoch Duration: {epoch_duartion:.1f} seconds')\n",
    "\n",
    "    train_duration = time.time() - train_start\n",
    "    print(f\"Training finished. Took {train_duration:.1f} seconds\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e454f580-881a-4064-b3ca-4a24ff51631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My classifier params: 89,605\n",
      "Torch classifier params: 89,605\n"
     ]
    }
   ],
   "source": [
    "def get_model_param_count(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f\"My classifier params: {get_model_param_count(my_classifier):,}\")\n",
    "print(f\"Torch classifier params: {get_model_param_count(torch_classifier):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1240444a-d8d5-46fd-b29b-3738f8ecc2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 1.5996, Validation Loss: 1.5754, Validation Accuracy: 0.3290. Epoch Duration: 8.5 seconds\n",
      "Epoch 3/10, Training Loss: 0.9998, Validation Loss: 0.7223, Validation Accuracy: 0.7622. Epoch Duration: 8.2 seconds\n",
      "Epoch 5/10, Training Loss: 0.4822, Validation Loss: 0.4587, Validation Accuracy: 0.8469. Epoch Duration: 8.2 seconds\n",
      "Epoch 7/10, Training Loss: 0.2149, Validation Loss: 0.3461, Validation Accuracy: 0.8795. Epoch Duration: 8.2 seconds\n",
      "Epoch 9/10, Training Loss: 0.0957, Validation Loss: 0.3107, Validation Accuracy: 0.9088. Epoch Duration: 8.3 seconds\n",
      "Epoch 10/10, Training Loss: 0.0662, Validation Loss: 0.2853, Validation Accuracy: 0.9153. Epoch Duration: 8.3 seconds\n",
      "Training finished. Took 82.4 seconds\n",
      "Epoch 1/10, Training Loss: 1.6022, Validation Loss: 1.5797, Validation Accuracy: 0.2410. Epoch Duration: 11.5 seconds\n",
      "Epoch 3/10, Training Loss: 0.9476, Validation Loss: 0.7273, Validation Accuracy: 0.6971. Epoch Duration: 11.6 seconds\n",
      "Epoch 5/10, Training Loss: 0.4713, Validation Loss: 0.4885, Validation Accuracy: 0.7980. Epoch Duration: 12.0 seconds\n",
      "Epoch 7/10, Training Loss: 0.3015, Validation Loss: 0.4381, Validation Accuracy: 0.8534. Epoch Duration: 11.5 seconds\n",
      "Epoch 9/10, Training Loss: 0.2112, Validation Loss: 0.3637, Validation Accuracy: 0.8730. Epoch Duration: 11.6 seconds\n",
      "Epoch 10/10, Training Loss: 0.1871, Validation Loss: 0.3382, Validation Accuracy: 0.8827. Epoch Duration: 11.7 seconds\n",
      "Training finished. Took 116.4 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch_losses = train(torch_classifier, train_dl, test_dl, epochs=10)\n",
    "my_losses = train(my_classifier, train_dl, test_dl, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "258997f4-a72c-442f-9092-afafff3a8b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.73      0.79        55\n",
      "           1       0.88      0.87      0.88        70\n",
      "           2       0.93      0.99      0.96        71\n",
      "           3       0.87      0.94      0.90        49\n",
      "           4       0.84      0.87      0.86        62\n",
      "\n",
      "    accuracy                           0.88       307\n",
      "   macro avg       0.88      0.88      0.88       307\n",
      "weighted avg       0.88      0.88      0.88       307\n",
      "\n",
      "Torch Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.90        55\n",
      "           1       0.93      0.94      0.94        70\n",
      "           2       0.95      0.97      0.96        71\n",
      "           3       0.91      0.84      0.87        49\n",
      "           4       0.91      0.85      0.88        62\n",
      "\n",
      "    accuracy                           0.92       307\n",
      "   macro avg       0.91      0.91      0.91       307\n",
      "weighted avg       0.92      0.92      0.91       307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import toolz\n",
    "import pandas as pd\n",
    "\n",
    "def predict(texts, model, bs=32):\n",
    "    output_dfs = []\n",
    "    for batch in toolz.partition_all(bs, texts):\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            class_probs = torch.softmax(model(**inputs), dim=1).numpy()\n",
    "            pred_classes = class_probs.argmax(axis=1)\n",
    "            col_names = [f\"class_{i}_prob\" for i in range(class_probs.shape[-1])]\n",
    "            df = pd.DataFrame(class_probs, columns=col_names)\n",
    "            df['pred_class'] = pred_classes\n",
    "            df['pred_class_name'] = df['pred_class'].map(class_id_to_class)\n",
    "            output_dfs.append(df)\n",
    "\n",
    "    return pd.concat(output_dfs)\n",
    "\n",
    "my_preds_df = predict(ds['test']['text'], my_classifier)\n",
    "my_preds_df['model'] = 'My Model'\n",
    "my_preds_df['actual_class'] = ds['test']['label']\n",
    "torch_preds_df = predict(ds['test']['text'], torch_classifier)\n",
    "torch_preds_df['model'] = 'Torch Model'\n",
    "torch_preds_df['actual_class'] = ds['test']['label']\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"My Classifier\")\n",
    "print(classification_report(my_preds_df['actual_class'], my_preds_df['pred_class']))\n",
    "\n",
    "print(\"Torch Classifier\")\n",
    "print(classification_report(torch_preds_df['actual_class'], torch_preds_df['pred_class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4c0b629-6e58-4247-815e-fe4da92a4b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEfficientMultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim: int, n_heads: int, projection_bias=False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % n_heads == 0, \"embed_dim must be divisible by n_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_embed_dim = self.embed_dim // n_heads\n",
    "        self.W_q = torch.nn.Linear(embed_dim, embed_dim, bias=projection_bias)\n",
    "        self.W_k = torch.nn.Linear(embed_dim, embed_dim, bias=projection_bias)\n",
    "        self.W_v = torch.nn.Linear(embed_dim, embed_dim, bias=projection_bias)\n",
    "        self.projection = torch.nn.Linear(embed_dim, embed_dim, bias=projection_bias)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # shape of query = (bs, seq_len, embed_dim)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # linear projection of query, key and value\n",
    "        q = self.W_q(query)\n",
    "        k = self.W_k(key)\n",
    "        v = self.W_v(value)\n",
    "\n",
    "        # reshape the projected query, key, value\n",
    "        # to (bs, n_heads, seq_len, head_embed_dim)\n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "\n",
    "        # do scaled dot product attention\n",
    "        # attn.shape = (bs, n_heads, seq_len, head_embed_dim)\n",
    "        # attn_weights.shape (bs, n_heads, seq_len, seq_len)\n",
    "        attn, attn_weights = my_scaled_dot_product_attention(q, k, v)\n",
    "        # swap the n_heads and seq_len so that we have\n",
    "        # (bs, seq_len, n_heads, head_embed_dim)\n",
    "        # call .contiguous() so that view function will work later\n",
    "        attn = attn.transpose(1, 2).contiguous()\n",
    "        # \"combine\" (n_heads, head_embed_dim) matrix as a single \"embed_dim\" vector\n",
    "        attn = attn.view(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        output = self.projection(attn)\n",
    "        return output, attn_weights.mean(dim=1)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x.shape = (bs, seq_len, embed_dim)\n",
    "        batch_size = x.size(0)\n",
    "        # first split the embed_dim into (n_heads, head_embed_dim)\n",
    "        temp =  x.view(batch_size, -1, self.n_heads, self.head_embed_dim)\n",
    "        # now we swap seq_len and n_heads dimension\n",
    "         # output shape = (bs, n_heads, seq_len, head_embed_dim)\n",
    "        return temp.transpose(1, 2)\n",
    "class MyEfficientMultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim: int, n_heads: int, projection_bias=False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % n_heads == 0, \"embed_dim must be divisible by n_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_embed_dim = self.embed_dim // n_heads\n",
    "        self.W_q = torch.nn.Linear(embed_dim, embed_dim, bias=projection_bias)\n",
    "        self.W_k = torch.nn.Linear(embed_dim, embed_dim, bias=projection_bias)\n",
    "        self.W_v = torch.nn.Linear(embed_dim, embed_dim, bias=projection_bias)\n",
    "        self.projection = torch.nn.Linear(embed_dim, embed_dim, bias=projection_bias)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # shape of query = (bs, seq_len, embed_dim)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # linear projection of query, key and value\n",
    "        q = self.W_q(query)\n",
    "        k = self.W_k(key)\n",
    "        v = self.W_v(value)\n",
    "\n",
    "        # reshape the projected query, key, value\n",
    "        # to (bs, n_heads, seq_len, head_embed_dim)\n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "\n",
    "        # do scaled dot product attention\n",
    "        # attn.shape = (bs, n_heads, seq_len, head_embed_dim)\n",
    "        # attn_weights.shape (bs, n_heads, seq_len, seq_len)\n",
    "        attn, attn_weights = my_scaled_dot_product_attention(q, k, v)\n",
    "        # swap the n_heads and seq_len so that we have\n",
    "        # (bs, seq_len, n_heads, head_embed_dim)\n",
    "        # call .contiguous() so that view function will work later\n",
    "        attn = attn.transpose(1, 2).contiguous()\n",
    "        # \"combine\" (n_heads, head_embed_dim) matrix as a single \"embed_dim\" vector\n",
    "        attn = attn.view(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        output = self.projection(attn)\n",
    "        return output, attn_weights.mean(dim=1)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x.shape = (bs, seq_len, embed_dim)\n",
    "        batch_size = x.size(0)\n",
    "        # first split the embed_dim into (n_heads, head_embed_dim)\n",
    "        temp =  x.view(batch_size, -1, self.n_heads, self.head_embed_dim)\n",
    "        # now we swap seq_len and n_heads dimension\n",
    "         # output shape = (bs, n_heads, seq_len, head_embed_dim)\n",
    "        return temp.transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ec8b6dc-a22e-419e-b779-a5a6e3674aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 1.6054, Validation Loss: 1.5861, Validation Accuracy: 0.4072. Epoch Duration: 9.8 seconds\n",
      "Epoch 3/10, Training Loss: 1.1183, Validation Loss: 0.7892, Validation Accuracy: 0.7068. Epoch Duration: 9.6 seconds\n",
      "Epoch 5/10, Training Loss: 0.4770, Validation Loss: 0.4058, Validation Accuracy: 0.8371. Epoch Duration: 9.6 seconds\n",
      "Epoch 7/10, Training Loss: 0.2780, Validation Loss: 0.4079, Validation Accuracy: 0.8339. Epoch Duration: 9.6 seconds\n",
      "Epoch 9/10, Training Loss: 0.2086, Validation Loss: 0.3294, Validation Accuracy: 0.8730. Epoch Duration: 9.6 seconds\n",
      "Epoch 10/10, Training Loss: 0.1688, Validation Loss: 0.3112, Validation Accuracy: 0.8958. Epoch Duration: 9.6 seconds\n",
      "Training finished. Took 96.5 seconds\n"
     ]
    }
   ],
   "source": [
    "my_efficient_mha = MyEfficientMultiHeadAttention(embed_dim=embed_dim, n_heads=n_heads, projection_bias=True)\n",
    "my_efficient_classifier = TextClassifier(vocab_size=tokenizer.vocab_size, embed_dim=embed_dim, num_classes=num_classes, mha=my_efficient_mha)\n",
    "my_efficient_losses = train(my_efficient_classifier, train_dl, test_dl, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03621532-6b04-4a30-b9c0-d23aa8593c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
